{"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"Based on 4chan NovelAILeaks (naifu) [link](https://boards.4channel.org/g/thread/89095460#p89097704)\n","metadata":{"id":"KZ88G-iWCTs7"}},{"cell_type":"markdown","source":"**adapted by physics1024 for kaggle, thanks for Anonymous, 炼铜术士, 神楽坂早苗, Jonathan, 咕 咕, 猫又逆变器, Gaein nidb, JingShing**","metadata":{}},{"cell_type":"code","source":"# Check GPU working status\n\n!nvidia-smi","metadata":{"id":"X5yF8TS1CR3L","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 1. Download Novel AI API backend, model\n# If the download speed is too slow try restart\n\n!mkdir ~/content\n%cd ~/content/\n!apt install -y aria2\n!aria2c --summary-interval=5 -x 3 --allow-overwrite=true -Z \\\n   https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/naifu.tar \\\n   https://pub-2fdef7a2969f43289c42ac5ae3412fd4.r2.dev/animefull-latest.tar\n  \n!echo \"Decompressing...\"\n!tar xf naifu.tar && rm -rf naifu.tar\n!echo \"Done.\"","metadata":{"id":"iqTO_Uf3F6VW","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 2. Install dependencies\n# Wait patiently for the installation to complete\n\n%cd ~/content/naifu\n!pip install virtualenv && bash ./setup.sh\n!aria2c -o bore.tar.gz https://github.com/ekzhang/bore/releases/download/v0.4.0/bore-v0.4.0-x86_64-unknown-linux-musl.tar.gz\n!tar zxf bore.tar.gz -C /usr/bin && rm -rf bore.tar.gz\n!aria2c -o cloudflared -d /usr/bin https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64 && chmod +x /usr/bin/cloudflared","metadata":{"id":"BysBfYRmGSo1","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**notice that if you want to run 7G version, *do not* run the cell beneath but the cell after it**","metadata":{}},{"cell_type":"code","source":"# 3. Start the model\n# Just visit the output mapping address (end with `trycloudflare.com` / `bore.pub`).\n# - Please wait until the model is loaded (`Application startup complete` appears) before accessing\n# - The service provided by cloudflare occasionally has a request timeout, which can be replaced by bore tunnel\n\n%cd ~/content/naifu\n!sed -i 's/# export SAVE_FILES=\"1\"/export SAVE_FILES=\"1\"/g' run.sh\n!bash run.sh & cloudflared tunnel --url localhost:6969","metadata":{"id":"uQBR9zXQGJrn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**the cell below is the 7G version**","metadata":{}},{"cell_type":"code","source":"# (Optional) Run with `animefull-latest` model of 7G\n# uses the 4G size animefull-final-pruned model by default. If you want to use the 7G animefull-latest model, run this\n\n%cd ~/content/\n!tar xf animefull-latest.tar -C ~/content/naifu/models && rm -rf animefull-latest.tar\n!sed -i 's/map_location=\"cpu\"/map_location=\"cuda\"/g' ~/content/naifu/hydra_node/models.py\n\n%cd ~/content/naifu\n%env DTYPE=float16\n%env CLIP_CONTEXTS=3\n%env AMP=1\n%env MODEL=stable-diffusion\n%env DEV=True\n%env MODEL_PATH=models/animefull-latest\n%env ENABLE_EMA=1\n%env VAE_PATH=models/animevae.pt\n%env PENULTIMATE=1\n%env PYTHONDONTWRITEBYTECODE=1\n%env SAVE_FILES=1\n\n!./venv/bin/python -m uvicorn --host 0.0.0.0 --port=6969 main:app & bore local 6969 --to bore.pub & cloudflared tunnel --url localhost:6969","metadata":{"id":"B9j9thAby5_2"},"execution_count":null,"outputs":[]}]}